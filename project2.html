<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Case Study: Dockerized Stock Data Pipeline</title>
    <link rel="stylesheet" href="style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;700&family=Inter:wght@400;600;800&display=swap" rel="stylesheet">
</head>
<body>

    <header>
        <nav>
            <div class="logo"><a href="index.html" style="text-decoration:none; color: inherit;">Gagan Gowda BG</a></div>
            <ul>
                <li><a href="index.html#projects">Back to Projects</a></li>
            </ul>
        </nav>
    </header>

    <main class="case-study">
        <div class="case-study-header">
            <h1>Dockerized Stock Data Pipeline (Airflow + PostgreSQL)</h1>
            <p class="tagline">A containerized ETL pipeline with Apache Airflow to fetch stock market data from the Alpha Vantage API and store it in PostgreSQL.</p>
        </div>

        <div class="case-study-content">
            <div class="content-section">
                <h2>The Problem</h2>
                <p>The objective was to complete a technical assignment that required the development of a fully automated, Dockerized data pipeline. The system needed to fetch stock market data from a public API on a regular schedule, parse the data, and reliably store it in a PostgreSQL database. The project emphasized robustness, requiring comprehensive error handling, scalability, and secure management of credentials.</p>
            </div>
            <div class="content-section">
                <h3>My Role</h3>
                <p>DAG development, pipeline integration, and Dockerization.</p>
            </div>
            <div class="content-section">
                <h3>Technologies Used</h3>
                <p>Python, Airflow, PostgreSQL, Docker, Git/GitHub.</p>
            </div>
            <div class="content-section">
                <h2>The Process & Solution</h2>
                <p>To meet the assignment's requirements, I chose to use Apache Airflow as the data orchestrator. The entire pipeline, including Airflow and a PostgreSQL database, was containerized using Docker Compose for seamless, single-command deployment. The core logic was encapsulated in an Airflow DAG named <code>stock_data_pipeline</code>, scheduled to run hourly. This DAG utilized a <code>PythonOperator</code> to execute a script that handled the full ETL process: fetching JSON data from the <strong>Alpha Vantage API</strong>, parsing the relevant information, and storing it in the PostgreSQL database. To ensure robustness, I added error handling using <strong>try-except blocks</strong> in the data fetching script to gracefully catch any API or database issues. I also utilized Airflowâ€™s built-in retry mechanism, setting tasks to automatically <strong>retry once on temporary failures</strong>. All sensitive information, such as API keys and database credentials, was securely managed using environment variables.</p>
            </div>
            <div class="content-section">
                <h2>Visual Evidence</h2>
                <p>The successful implementation and operation of the DAG are demonstrated in the Airflow UI, showing the pipeline's structure and successful runs.</p>
                <div class="image-gallery">
                    <img src="airflow-dashboard.png" alt="Airflow dashboard showing the active data pipeline DAG">
                    <img src="airflow-dag-graph.png" alt="Airflow graph view showing the pipeline's single task">
                </div>
            </div>
        </div>
    </main>

    <footer id="contact" style="background: #fff; border-top: 1px solid var(--border-color);">
        <p style="color: var(--text-color);">Gagan Gowda BG | 2025</p>
    </footer>

</body>
</html>
